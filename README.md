# Fake News Classification # 

For this homework, I chose to study and implement Yoon Kim’s “Convolutional Neural Networks for Sentence Classification.” (http://www.aclweb.org/anthology/D14-1181) 

	The technical approach takes on an embedding layer, convolutional neural network, max pooling, dropout, and eventually a softmax layer to classify between the 6 labels of pants-fire, false, barely-true, half-true, mostly-true, and true. I implemented my deep learning architecture using Python 2.7 and the Keras deep learning framework. 

	Before feeding data into our model, I had to clean it up and tokenize into a matrix that the CNN would be able to recognize. I initially used the python built in package csv to read our data, but for some reason it skipped some lines of data while reading the .tsv files. To fix this, I just split and stripped each line of the train file with a ‘\t’ delimited as opposed to using the csv package and specifying the delimiter as ‘\t’. For each row, I used one-hot encoding to encode what label the row of data corresponded with based on the increasing scale from pants-fire to true. In terms of the data, I used Yoon Kim’s preprocessing script and cleaned up the data of any special characters such as “that’ll” to “that \’ll” so then “\’ll” gets tokenizes as a separate term. Similarly, this is applied to other special characters such as from “?” to “ \? “. I then pad each sentence to be the same length as the maximum sentence in the dataset with the padding token “<PAD/>”

	Model wise, our model consists of an embedding layer that takes the inputs and passes them out into a size of 256. It is then reshaped into a tensor of shape (max_sentence_length, embedding_dim, 1) to be fed into the convolutional layer. Just as the Kim’s paper did, they decided to use filter windows of 3, 4, and 5. This calls for us to take our tensor and pass it into 3 separate convolutional layers and max pooling and then concatenating them back together. I let the size of the output dimensions of each 3 of our convolutional layers be 512, double the size of our embedding outputs. I set the kernel size to be either of the 3 filter windows by the embedding dimension with a standard normal initialization. I use normal padding and ReLU activation for each convolutional layer as stated by the paper. I then pass each respective output into a max pooling layer with similarly valid padding and strides of (1,1). I then concatenate the outputs of the 3 max pooling layers and flatten it. Afterwards, I perform dropout with a dropout constant of 0.5 before passing it into a softmax layer of 6 possible classification outputs. 

For training, I use an Adam optimizer function with an initial learning rate of 1e-4, both betas being 0.9 and 0.999 respectively (as they should be near 1 but <1), and a decay of 0.0. When I compile the model, I define our loss function to be the cross entropy. Once our model is trained, I test it against the test set and save our predictions to ‘predictions.txt’. Although the Adam optimizer is still relatively new and there isn’t quite an industry standard with the decay factor yet, I did not see much of a difference in using decay on my Adam optimizer. 

Experimental settings wise, I’ve been toying around with the hyperparameters of the model. I toyed around with different epochs (50 and 100), learning rates (1e-3 and 1e-4), batch sizes (20-50), and even changed the filter window sizes from what was specified in Kim’s paper to [4,5,6]. However, despite my efforts in changing these hyperparameters, my accuracy still hovered around 18 and 19%. I wanted to implement a Bi-LSTM as well to track the different parties and speakers to see if there was a relationship there too towards fake news spreading, but did not have time due to work and school piling all over each other. 

